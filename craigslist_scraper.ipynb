{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import requests\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper 1\n",
    "\n",
    "Fast craigslist scraper. Only gets price, size, title, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://sfbay.craigslist.org/search/eby/apa?postal=94720&search_distance=4\n"
     ]
    }
   ],
   "source": [
    "url_base = 'http://sfbay.craigslist.org/search/eby/apa'\n",
    "params = dict(search_distance=4, postal=94720)\n",
    "rsp = requests.get(url_base, params=params)\n",
    "html = bs4(rsp.text, 'html.parser')\n",
    "apts = html.find_all('p', attrs={'class': 'row'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "cl_data = []\n",
    "for i in [0,100,200,300,400,500,600,700,800,900,1000,1100]:\n",
    "    params = dict(search_distance=4, postal=94720,s=i)\n",
    "    rsp = requests.get(url_base, params=params)\n",
    "    html = bs4(rsp.text, 'html.parser')\n",
    "    apts = html.find_all('p', attrs={'class': 'row'})\n",
    "    for apt in apts:\n",
    "        url = \"https://sfbay.craigslist.org\" + apt.find('a', attrs={'class': 'hdrlnk'})['href']\n",
    "        try:\n",
    "            size = apt.findAll(attrs={'class': 'housing'})[0].text\n",
    "        except IndexError:\n",
    "            size = \"Not Listed\"\n",
    "        title = apt.find('a',attrs={'class': 'hdrlnk'}).text\n",
    "        try:\n",
    "            price = apt.findAll(attrs={'class': 'price'})[0].text\n",
    "        except IndexError:\n",
    "            price = \"Not Listed\"\n",
    "        location = apt.findAll(attrs={'class': 'pnr'})[0].text\n",
    "        #print url,size,title,price,location\n",
    "        cl_string = url + \",\" + size + \",\" + title + \",\" + price + \",\" + location + \"\\n\"\n",
    "        cl_data.append(cl_string)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "f1=open('cl.csv', 'w+')\n",
    "f1.write('url,size,title,price,location\\n')\n",
    "for data in cl_data:\n",
    "    try:\n",
    "        f1.write(data)\n",
    "    except:\n",
    "        pass\n",
    "f1.close()\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scraper 2\n",
    "\n",
    "More thorough, grabs size, price, city, lat/long, features, open house, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time, json\n",
    "cl_data = []\n",
    "for i in [400,500,600,700,800,900,1000]:\n",
    "    time.sleep(3)\n",
    "    url_base = 'http://sfbay.craigslist.org/search/eby/apa'\n",
    "    params = dict(search_distance=4, postal=94720,s=i)\n",
    "    rsp = requests.get(url_base, params=params)\n",
    "    html = bs4(rsp.text, 'html.parser')\n",
    "    apts = html.find_all('p', attrs={'class': 'row'})\n",
    "    #for apt in apts:\n",
    "    data = {}\n",
    "    for apt in apts:\n",
    "        time.sleep(1)\n",
    "        url = \"https://sfbay.craigslist.org\" + apt.find('a', attrs={'class': 'hdrlnk'})['href']\n",
    "        r = urllib.urlopen(url).read()\n",
    "        soup = bs4(r)\n",
    "        final_dict = {}\n",
    "        title = soup.findAll(\"span\", {\"id\": \"titletextonly\"})[0].text\n",
    "        try:\n",
    "            size = soup.find(\"span\", {\"class\": \"housing\"}).text\n",
    "        except:\n",
    "            size = \"n/a\"\n",
    "        try:\n",
    "            price = soup.findAll(\"span\", {\"class\": \"price\"})[0].text\n",
    "        except:\n",
    "            price = \"n/a\"\n",
    "        try:\n",
    "            city = soup.findAll(\"small\")[0].text\n",
    "        except:\n",
    "            city = \"n/a\"\n",
    "        try:\n",
    "            longitude =  soup.findAll(\"div\", {\"class\": \"viewposting\"})[0]['data-longitude']\n",
    "            latitude = soup.findAll(\"div\", {\"class\": \"viewposting\"})[0]['data-latitude']\n",
    "        except:\n",
    "            longitude = \"n/a\"\n",
    "            latitude = \"n/a\"\n",
    "        try:\n",
    "            features = soup.find(id='postingbody').text\n",
    "        except:\n",
    "            features = \"n/a\"\n",
    "        try:\n",
    "            open_house = soup.find(\"span\", {\"class\": \"otherpostings\"}).text\n",
    "        except:\n",
    "            open_house = \"n/a\"\n",
    "        images = []\n",
    "        gmap = \"n/a\"\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if \"images.craigslist.org\" in a['href']:\n",
    "                images.append(a['href'])\n",
    "            if \"maps.google.com\" in a['href']:\n",
    "                gmap = a['href']\n",
    "        final_dict['title'] = title\n",
    "        final_dict['price'] = price\n",
    "        final_dict['city'] = city\n",
    "        final_dict['longitude'] = longitude\n",
    "        final_dict['latitude'] = latitude\n",
    "        final_dict['features'] = features\n",
    "        final_dict['open_house'] = open_house\n",
    "        final_dict['images'] = images\n",
    "        final_dict['gmap'] = gmap\n",
    "        final_dict['size'] = size\n",
    "\n",
    "        data[url] = final_dict\n",
    "    \n",
    "    filename = \"data\" + str(i) + \".json\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
